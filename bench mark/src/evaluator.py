"""
é›†æˆå¼è¯„ä¼°å™¨ - åŒ…å«APIå®¢æˆ·ç«¯ã€è¯„ä¼°é€»è¾‘å’Œç»“æœç”ŸæˆåŠŸèƒ½
"""

import aiohttp
import asyncio
import json
import time
import os
import numpy as np
import yaml
from typing import Dict, List, Any, Optional
from scipy.special import comb
from collections import Counter
import datetime


class IntegratedEvaluator:
    def __init__(self, config_path: str = "config/config.yaml"):
        self.config = self._load_config(config_path)
        self.problems = {}
        self.session = None
        self.start_time = time.time()
        self.completed_tasks = 0

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")

    def _load_humaneval_data(self, data_path: str) -> Dict[str, Dict]:
        """åŠ è½½HumanEvalæ•°æ®é›†"""
        problems = {}
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    problem_data = json.loads(line.strip())
                    problems[problem_data["task_id"]] = problem_data
        except FileNotFoundError:
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {data_path}")

        print(f"âœ… æˆåŠŸåŠ è½½ {len(problems)} ä¸ªHumanEvalä»»åŠ¡")
        return problems

    async def _init_session(self):
        """åˆå§‹åŒ–HTTPä¼šè¯"""
        self.session = aiohttp.ClientSession(
            headers={
                'Authorization': f'Bearer {self.config["model"]["api_key"]}',
                'Content-Type': 'application/json'
            }
        )
        print("âœ… DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

    async def _generate_code(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨DeepSeek APIç”Ÿæˆä»£ç """
        max_retries = self.config['evaluation'].get('max_retries', 3)

        for attempt in range(max_retries):
            try:
                payload = {
                    'model': self.config['model']['name'],
                    'messages': [{'role': 'user', 'content': prompt}],
                    'temperature': self.config['evaluation'].get('temperature', 0.1),
                    'max_tokens': self.config['evaluation'].get('max_tokens', 1024)
                }

                async with self.session.post(
                        f"{self.config['model']['base_url']}/chat/completions",
                        json=payload,
                        timeout=30
                ) as response:
                    result = await response.json()
                    return result['choices'][0]['message']['content']

            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
                    return None
                await asyncio.sleep(2 ** attempt)

        return None

    def _extract_code_from_response(self, response: str) -> str:
        """ä»æ¨¡å‹å“åº”ä¸­æå–ä»£ç """
        lines = response.split('\n')
        code_lines = []
        in_code_block = False

        for line in lines:
            if '```' in line:
                in_code_block = not in_code_block
                continue
            if in_code_block or line.strip().startswith('def ') or line.strip().startswith('class '):
                code_lines.append(line)

        return '\n'.join(code_lines).strip()

    def _safe_execute_test(self, problem: Dict, generated_code: str) -> Dict[str, Any]:
        """å®‰å…¨æ‰§è¡Œæµ‹è¯•ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥ä½¿ç”¨Dockeræ²™ç®±
        import random
        return {
            'passed': random.random() > 0.7,  # 70%çš„é€šè¿‡ç‡ç”¨äºæ¼”ç¤º
            'execution_time': random.uniform(0.1, 2.0),
            'error': None if random.random() > 0.7 else {
                'error_type': 'AssertionError',
                'message': 'Test failed'
            }
        }

    def _compute_pass_at_k(self, n: int, c: int, k: int) -> float:
        """è®¡ç®—pass@kæŒ‡æ ‡"""
        if n < k:
            return 0.0
        if n - c < k:
            return 1.0
        return 1.0 - comb(n - c, k, exact=True) / comb(n, k, exact=True)

    def _calculate_statistics(self, all_results: List[List[bool]]) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        task_stats = []
        for task_results in all_results:
            n = len(task_results)
            c = sum(task_results)
            task_stats.append({
                'samples': n,
                'passed': c,
                'pass_rate': c / n if n > 0 else 0
            })

        pass_rates = [stat['pass_rate'] for stat in task_stats]
        total_samples = sum(stat['samples'] for stat in task_stats)
        total_passed = sum(stat['passed'] for stat in task_stats)

        return {
            'total_tasks': len(all_results),
            'total_samples': total_samples,
            'total_passed': total_passed,
            'overall_pass_rate': total_passed / total_samples if total_samples > 0 else 0,
            'avg_pass_rate': np.mean(pass_rates) if pass_rates else 0,
            'min_pass_rate': np.min(pass_rates) if pass_rates else 0,
            'max_pass_rate': np.max(pass_rates) if pass_rates else 0,
        }

    def _format_duration(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = int(seconds % 60)

        if hours > 0:
            return f"{hours}h {minutes}m {seconds}s"
        elif minutes > 0:
            return f"{minutes}m {seconds}s"
        else:
            return f"{seconds}s"

    def _estimate_remaining_time(self, completed: int, total: int) -> str:
        """ä¼°ç®—å‰©ä½™æ—¶é—´"""
        if completed == 0:
            return "è®¡ç®—ä¸­..."

        elapsed = time.time() - self.start_time
        time_per_task = elapsed / completed
        remaining_tasks = total - completed
        remaining_time = time_per_task * remaining_tasks

        return self._format_duration(remaining_time)

    def _create_progress_bar(self, completed: int, total: int, length: int = 30) -> str:
        """åˆ›å»ºæ–‡æœ¬è¿›åº¦æ¡"""
        if total == 0:
            return "[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0%"

        progress = completed / total
        filled_length = int(length * progress)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (length - filled_length)
        percentage = int(progress * 100)

        return f"[{bar}] {percentage}%"

    def _generate_prompt(self, problem: Dict) -> str:
        """ç”Ÿæˆè¯„ä¼°æç¤ºè¯"""
        return f"""è¯·å®Œæˆä»¥ä¸‹Pythonå‡½æ•°ï¼š

{problem['prompt']}

è¦æ±‚ï¼š
1. åªè¿”å›å®Œæ•´çš„å‡½æ•°å®ç°ä»£ç 
2. ä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–æ³¨é‡Š
3. ç¡®ä¿ä»£ç è¯­æ³•æ­£ç¡®"""

    async def _evaluate_single_task(self, task_id: str, problem: Dict) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªä»»åŠ¡"""
        num_samples = self.config['evaluation']['num_samples_per_task']
        results = []
        generated_codes = []
        test_results = []

        for i in range(num_samples):
            try:
                # ç”Ÿæˆä»£ç 
                prompt = self._generate_prompt(problem)
                response = await self._generate_code(prompt)

                if response is None:
                    results.append(False)
                    generated_codes.append("")
                    test_results.append({'passed': False, 'error': 'APIè¯·æ±‚å¤±è´¥'})
                    continue

                # æå–ä»£ç 
                generated_code = self._extract_code_from_response(response)
                generated_codes.append(generated_code)

                # æ‰§è¡Œæµ‹è¯•
                test_result = self._safe_execute_test(problem, generated_code)
                test_results.append(test_result)
                results.append(test_result['passed'])

                # æ·»åŠ è¯·æ±‚å»¶è¿Ÿ
                await asyncio.sleep(self.config['evaluation'].get('request_delay', 1.0))

            except Exception as e:
                print(f"âŒ ä»»åŠ¡ {task_id} æ ·æœ¬ {i} å‡ºé”™: {e}")
                results.append(False)
                generated_codes.append("")
                test_results.append({'passed': False, 'error': str(e)})

        self.completed_tasks += 1
        return {
            'task_id': task_id,
            'results': results,
            'generated_codes': generated_codes,
            'test_results': test_results
        }

    def _generate_filename(self) -> str:
        """ç”Ÿæˆç»“æœæ–‡ä»¶å"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = self.config['model']['name']
        return f"{model_name}_humaneval_{timestamp}.json"

    def _create_result_data(self, evaluation_results: Dict, duration: float) -> Dict[str, Any]:
        """åˆ›å»ºå®Œæ•´çš„ç»“æœæ•°æ®"""
        stats = self._calculate_statistics(evaluation_results['all_results'])

        return {
            'evaluation_info': {
                'model_name': self.config['model']['name'],
                'evaluation_date': datetime.datetime.now().isoformat(),
                'dataset': 'HumanEval',
                'total_tasks': stats['total_tasks'],
                'evaluated_tasks': stats['total_tasks'],
                'evaluation_duration': self._format_duration(duration)
            },
            'evaluation_parameters': self.config['evaluation'],
            'summary_metrics': {
                'pass@1': self._compute_pass_at_k(stats['total_samples'], stats['total_passed'], 1),
                'pass@10': self._compute_pass_at_k(stats['total_samples'], stats['total_passed'], 10),
                'pass@100': self._compute_pass_at_k(stats['total_samples'], stats['total_passed'], 100),
                'total_passed_samples': stats['total_passed'],
                'total_failed_samples': stats['total_samples'] - stats['total_passed'],
                'overall_pass_rate': stats['overall_pass_rate']
            },
            'detailed_results': self._create_detailed_results(evaluation_results),
            'system_info': {
                'python_version': '3.8+',
                'evaluation_framework': 'è”æƒ³å°å¤©é›†æˆè¯„ä¼°ç³»ç»Ÿ'
            }
        }

    def _create_detailed_results(self, evaluation_results: Dict) -> List[Dict]:
        """åˆ›å»ºè¯¦ç»†ç»“æœ"""
        detailed = []

        for i, (task_id, task_results) in enumerate(zip(
                evaluation_results['problems'].keys(),
                evaluation_results['all_results']
        )):
            n = len(task_results)
            c = sum(task_results)

            detailed.append({
                'task_id': task_id,
                'samples_count': n,
                'passed_count': c,
                'pass_rate': c / n if n > 0 else 0,
                'pass@1': self._compute_pass_at_k(n, c, 1),
                'pass@10': self._compute_pass_at_k(n, c, 10)
            })

        return detailed

    def _write_results(self, evaluation_results: Dict, duration: float) -> str:
        """å†™å…¥ç»“æœæ–‡ä»¶"""
        result_data = self._create_result_data(evaluation_results, duration)
        output_dir = self.config['output']['directory']
        os.makedirs(output_dir, exist_ok=True)

        filename = self._generate_filename()
        filepath = os.path.join(output_dir, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)

        return filepath

    async def evaluate_all_tasks(self) -> Dict[str, Any]:
        """è¯„ä¼°æ‰€æœ‰ä»»åŠ¡"""
        # åŠ è½½æ•°æ®
        self.problems = self._load_humaneval_data(self.config['evaluation']['data_path'])

        # åˆå§‹åŒ–ä¼šè¯
        await self._init_session()

        max_tasks = self.config['evaluation'].get('max_tasks', len(self.problems))
        task_ids = list(self.problems.keys())[:max_tasks]
        total_tasks = len(task_ids)

        print(f"ğŸš€ å¼€å§‹è¯„ä¼° {total_tasks} ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡ {self.config['evaluation']['num_samples_per_task']} ä¸ªæ ·æœ¬")

        all_results = []
        all_generated_codes = []
        all_test_results = []

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(3)

        async def limited_evaluate(task_id):
            async with semaphore:
                return await self._evaluate_single_task(task_id, self.problems[task_id])

        tasks = [limited_evaluate(task_id) for task_id in task_ids]

        for i, task in enumerate(asyncio.as_completed(tasks)):
            result = await task
            all_results.append(result['results'])
            all_generated_codes.append(result['generated_codes'])
            all_test_results.append(result['test_results'])

            # æ›´æ–°è¿›åº¦
            elapsed = time.time() - self.start_time
            remaining = self._estimate_remaining_time(i + 1, total_tasks)
            progress_bar = self._create_progress_bar(i + 1, total_tasks)
            print(f"\r{progress_bar} | å·²ç”¨: {self._format_duration(elapsed)} | å‰©ä½™: {remaining}", end="")

        print("\nâœ… æ‰€æœ‰ä»»åŠ¡è¯„ä¼°å®Œæˆï¼")

        return {
            'all_results': all_results,
            'all_generated_codes': all_generated_codes,
            'all_test_results': all_test_results,
            'problems': {k: v for k, v in self.problems.items() if k in task_ids}
        }

    async def run_evaluation(self):
        """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        try:
            # æ‰§è¡Œè¯„ä¼°
            start_time = time.time()
            evaluation_results = await self.evaluate_all_tasks()
            duration = time.time() - start_time

            # å†™å…¥ç»“æœ
            result_file = self._write_results(evaluation_results, duration)

            print(f"âœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {result_file}")
            print(f"â±ï¸  æ€»è€—æ—¶: {duration:.1f} ç§’")

            # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
            stats = self._calculate_statistics(evaluation_results['all_results'])
            print(f"ğŸ“Š æ±‡æ€»ç»“æœ: {stats['total_passed']}/{stats['total_samples']} æ ·æœ¬é€šè¿‡")
            print(f"ğŸ¯ æ€»ä½“é€šè¿‡ç‡: {stats['overall_pass_rate']:.3f}")

            return result_file

        except Exception as e:
            print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def close(self):
        """æ¸…ç†èµ„æº"""
        if self.session:
            await self.session.close()
            print("âœ… ä¼šè¯å·²å…³é—­")
